# -*- coding: utf-8 -*-
"""01_add_subtract_multiply_divide.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X66_ka99u6dFAkP26giiQHz2WVjObvIL
"""

import torch
tensor = torch.tensor([1,2,3])
tensor + 10
torch.add(tensor, 10)
tensor * tensor
torch.matmul(tensor, tensor)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# value = 0
# for i in range(len(tensor)):
#   value += tensor[i] * tensor[i]
# print(value)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# torch.matmul(tensor, tensor)
# print(value)

tensor_A = torch.tensor([[1, 2],[3, 4],[5, 6]])
tensor_B = torch.tensor([[7, 10, 10], [3, 4, 34], [5, 6, 3]])
torch.mm(tensor_B, tensor_A)
torch.mm(tensor_A.T, tensor_B)
torch.min(tensor_A)
torch.max(tensor_B)
torch.sum(tensor_B)
torch.mean(tensor_B.type(torch.float))
tensor_A.argmin()
tensor_A.argmax()

x = torch.arange(0, 10)
x, x.shape
x = x.reshape(2, 5)
x.view(1, 10)
x_stacked = torch.stack([x, x, x, x])
x_stacked.shape
x_stacked = torch.stack([x, x, x, x], dim=2)
x_stacked.shape
x.reshape(1, 10)
torch.squeeze(x.reshape(1, 10)).shape
torch.unsqueeze(torch.arange(0, 10), dim=0).shape
torch.arange(0, 10).shape
x_original = torch.rand(size=(224, 220, 3))
x_permuted = x_original.permute(2, 0, 1)
print(x_original.shape, x_permuted.shape)
x_original[0, 0, 0], x_permuted[0,0,0]
x_original[0, 0, 1], x_permuted[0,0,1]
x_original[0, 0, 0]
x_original[0, 0, 0] = 2
x_original[0, 0, 0], x_permuted[0, 0, 0]